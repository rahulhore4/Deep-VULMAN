{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d849053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7422d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## State extraction function\n",
    "\n",
    "# Function to Extract the state space out of the environment given number of critical present at that state, \n",
    "#total expected critical, the current week and the amount of time left\n",
    "\n",
    "def arrival_state_sim(n_critical,current_week, time_available):\n",
    "    state = []\n",
    "    \n",
    "\n",
    "    c_mit_time = n_critical*150\n",
    "    state.append(c_mit_time)\n",
    "\n",
    "\n",
    "\n",
    "    state.append(current_week)\n",
    "\n",
    "    state.append(time_available)\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d21e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function scalarizes any value from one range to another.\n",
    "def action_scalarization(oldactionmin,oldactionmax,newactionmin,newactionmax,action):\n",
    "    oldrange = oldactionmax - oldactionmin\n",
    "    newrange = newactionmax - newactionmin\n",
    "    newaction = (((action - oldactionmin)* newrange)/oldrange) + newactionmin\n",
    "    return newaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action, n_crit_next,current_week):\n",
    "    \n",
    "    ## Scalarizes the action from range(-1,1) to (0,9600) minutes\n",
    "    action_minutes =  int(action_scalarization(-1,1,600*4,3600*4,action))\n",
    "    \n",
    "    action_reward = - action_minutes * 0.00001 \n",
    "    \n",
    "#     action_reward = 1 - (abs(action_minutes - 2400) / 2400)\n",
    "\n",
    "    \n",
    "    ## Reward and penalty for taking an action that is more than the remaining time\n",
    "    flag = False\n",
    "    if action_minutes > state[-1]:\n",
    "        reward_leg = 0\n",
    "        action_minutes = state[-1]\n",
    "        flag = True\n",
    "        \n",
    "    ## Calculating the new remaining time for the next step\n",
    "    remaining_time = state[-1] - action_minutes\n",
    "    \n",
    "    if state[0] > 0:\n",
    "        if action_minutes > state[0]:\n",
    "            crit_reward = min(1, action_minutes / state[0])\n",
    "            rem_crit_minute = 0\n",
    "        else:\n",
    "            crit_reward = action_minutes / state[0]\n",
    "            rem_crit_minute = state[0] - action_minutes\n",
    "    else:\n",
    "        crit_reward = 0\n",
    "        rem_crit_minute = 0 \n",
    "\n",
    "    ### Generating reward\n",
    "    reward =  0.8 *crit_reward + 0.2 *action_reward\n",
    "    \n",
    "    ### Generating next state\n",
    "    crit_mins_ns = rem_crit_minute + n_crit_next*150\n",
    "    next_state = [crit_mins_ns, current_week+1, remaining_time ]\n",
    "    \n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7a9fa9",
   "metadata": {},
   "source": [
    "## The PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55138e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Import libraries ###############################\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import pybullet_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce64fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## set device ##################################\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "# set device to cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if(torch.cuda.is_available()): \n",
    "    device = torch.device('cuda:0') \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")\n",
    "    \n",
    "print(\"============================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3755ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## PPO Policy ##################################\n",
    "\n",
    "\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_dim = action_dim\n",
    "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
    "\n",
    "        # actor\n",
    "        if has_continuous_action_space :\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Tanh()\n",
    "                        )\n",
    "        else:\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Softmax(dim=-1)\n",
    "                        )\n",
    "\n",
    "        \n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 1)\n",
    "                    )\n",
    "        \n",
    "    def set_action_std(self, new_action_std):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "        \n",
    "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        \n",
    "        return action.detach(), action_logprob.detach()\n",
    "    \n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            action_var = self.action_var.expand_as(action_mean)\n",
    "            cov_mat = torch.diag_embed(action_var).to(device)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "            \n",
    "            # for single action continuous environments\n",
    "            if self.action_dim == 1:\n",
    "                action = action.reshape(-1, self.action_dim)\n",
    "\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "        \n",
    "        return action_logprobs, state_values, dist_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e65f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_std = action_std_init\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                    ])\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        \n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.policy.set_action_std(new_action_std)\n",
    "            self.policy_old.set_action_std(new_action_std)\n",
    "        \n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = round(self.action_std, 4)\n",
    "            if (self.action_std <= min_action_std):\n",
    "                self.action_std = min_action_std\n",
    "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
    "            else:\n",
    "                print(\"setting actor output action_std to : \", self.action_std)\n",
    "            self.set_action_std(self.action_std)\n",
    "\n",
    "        else:\n",
    "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
    "\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob = self.policy_old.act(state)\n",
    "\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "\n",
    "            return action.detach().cpu().numpy().flatten()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob = self.policy_old.act(state)\n",
    "            \n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "\n",
    "            return action.item()\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "            \n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "\n",
    "        \n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss\n",
    "            advantages = rewards - state_values.detach()   \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    \n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "   \n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa21d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "################################### Training ###################################\n",
    "\n",
    "\n",
    "####### initialize environment hyperparameters ######\n",
    "\n",
    "env_name = 'Dynamic-VI-PPO'\n",
    "has_continuous_action_space = True\n",
    "\n",
    "max_ep_len = 4                   # max timesteps in one episode\n",
    "max_training_timesteps = int(80000)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
    "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
    "save_model_freq = int(2e3)      # save model frequency (in num timesteps)\n",
    "\n",
    "action_std = 0.6                    # starting std for action distribution (Multivariate Normal)\n",
    "action_std_decay_rate = 0.05        # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
    "min_action_std = 0.1                # minimum action_std (stop decay after action_std <= min_action_std)\n",
    "action_std_decay_freq = int(4e3)  # action_std decay frequency (in num timesteps)\n",
    "\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "## Note : print/log frequencies should be > than max_ep_len\n",
    "\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "\n",
    "\n",
    "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
    "K_epochs = 40               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0002      # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0         # set random seed if required (0 = no random seed)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "\n",
    "print(\"training environment name : \" + env_name)\n",
    "\n",
    "# env = gym.make(env_name)\n",
    "\n",
    "# state space dimension\n",
    "state_dim = 3\n",
    "\n",
    "# action space dimension\n",
    "if has_continuous_action_space:\n",
    "    action_dim = 1\n",
    "else:\n",
    "    action_dim = 1\n",
    "\n",
    "\n",
    "\n",
    "###################### logging ######################\n",
    "\n",
    "#### log files for multiple runs are NOT overwritten\n",
    "\n",
    "log_dir = \"PPO_logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "log_dir = log_dir + '/' + env_name + '/'\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "\n",
    "#### get number of log files in log directory\n",
    "run_num = 0\n",
    "current_num_files = next(os.walk(log_dir))[2]\n",
    "run_num = len(current_num_files)\n",
    "\n",
    "\n",
    "#### create new log file for each run \n",
    "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
    "\n",
    "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
    "print(\"logging at : \" + log_f_name)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "################### checkpointing ###################\n",
    "\n",
    "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
    "\n",
    "directory = \"PPO_preTrained\"\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "directory = directory + '/' + env_name + '/'\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "\n",
    "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "print(\"save checkpoint path : \" + checkpoint_path)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "############# print all hyperparameters #############\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"max training timesteps : \", max_training_timesteps)\n",
    "print(\"max timesteps per episode : \", max_ep_len)\n",
    "\n",
    "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
    "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
    "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"state space dimension : \", state_dim)\n",
    "print(\"action space dimension : \", action_dim)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "if has_continuous_action_space:\n",
    "    print(\"Initializing a continuous action space policy\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"starting std of action distribution : \", action_std)\n",
    "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
    "    print(\"minimum std of action distribution : \", min_action_std)\n",
    "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
    "\n",
    "else:\n",
    "    print(\"Initializing a discrete action space policy\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\") \n",
    "print(\"PPO K epochs : \", K_epochs)\n",
    "print(\"PPO epsilon clip : \", eps_clip)\n",
    "print(\"discount factor (gamma) : \", gamma)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"optimizer learning rate actor : \", lr_actor)\n",
    "print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "if random_seed:\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"setting random seed to \", random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "################# training procedure ################\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "\n",
    "# track total training time\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "# logging file\n",
    "log_f = open(log_f_name,\"w+\")\n",
    "log_f.write('episode,timestep,reward\\n')\n",
    "\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "log_running_reward = 0\n",
    "log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "reward_list = []\n",
    "episode_list = []\n",
    "# training loop\n",
    "total_expected_critical = 50\n",
    "epi_no = 0\n",
    "while time_step <= max_training_timesteps:\n",
    "    \n",
    "\n",
    "    crit_week1 = [random.randint(4,20), random.randint(40,60), random.randint(80,100), random.randint(4,20),0]\n",
    "    crit_week2 = [ random.randint(40,60),random.randint(4,20),  random.randint(4,20),random.randint(80,100),0]\n",
    "    crit_week3 = [random.randint(4,20), random.randint(4,20),  random.randint(80,100),random.randint(40,60),0]\n",
    "    crit_week4 = [random.randint(80,100), random.randint(4,20), random.randint(40,60),  random.randint(4,20),0]\n",
    "    \n",
    "    crit_pat_sel = [crit_week1,crit_week2,crit_week3,crit_week4]\n",
    "    \n",
    "    crit_arrival_list = random.choice(crit_pat_sel)\n",
    "    \n",
    "    init_arrival_crit = crit_arrival_list[0]\n",
    "    \n",
    "\n",
    "    \n",
    "    state_l = [init_arrival_crit*150 , 0, 9600*4]\n",
    "    \n",
    "    \n",
    "    state = np.array(state_l)\n",
    "    \n",
    "    done = False\n",
    "    current_ep_reward = 0\n",
    "    print('EPISODE')\n",
    "    print(epi_no)\n",
    "    print('----------------------')\n",
    "    for t in range(0, max_ep_len):\n",
    "        print('State')\n",
    "        print(state)\n",
    "        print('----------------------')\n",
    "        \n",
    "        # select action with policy\n",
    "        action = ppo_agent.select_action(state)\n",
    "        action =  action.clip(-1,1)\n",
    "        print('Action')\n",
    "        print(action)\n",
    "        print('in minutes', action_scalarization(-1,1,600*4,3600*4,action[0]))\n",
    "        print('----------------------')\n",
    "        \n",
    "        n_critical_next = crit_arrival_list[t+1]\n",
    "\n",
    "\n",
    "        \n",
    "        next_state_l, reward = step(state_l, action[0], n_critical_next,t)\n",
    "        next_state = np.array(next_state_l)\n",
    "        state_l = next_state_l\n",
    "        state = np.array(state_l)\n",
    "        \n",
    "        print('next_state')\n",
    "        print(next_state)\n",
    "        print('----------------------')\n",
    "        \n",
    "        print('Reward')\n",
    "        print(reward)\n",
    "        print('----------------------')\n",
    "        \n",
    "        if t == max_ep_len - 1:\n",
    "            done = True\n",
    "            epi_no += 1\n",
    "        \n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "        \n",
    "        time_step +=1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update()\n",
    "\n",
    "        # if continuous action space; then decay action std of ouput action distribution\n",
    "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
    "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "        # log in logging file\n",
    "        if time_step % log_freq == 0:\n",
    "\n",
    "            # log average reward till last episode\n",
    "            log_avg_reward = log_running_reward / log_running_episodes\n",
    "            log_avg_reward = round(log_avg_reward, 4)\n",
    "\n",
    "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
    "            log_f.flush()\n",
    "\n",
    "            log_running_reward = 0\n",
    "            log_running_episodes = 0\n",
    "\n",
    "        # printing average reward\n",
    "        if time_step % print_freq == 0:\n",
    "\n",
    "            # print average reward till last episode\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "            \n",
    "        # save model weights\n",
    "        if time_step % save_model_freq == 0:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"saving model at : \" + checkpoint_path)\n",
    "            ppo_agent.save(checkpoint_path)\n",
    "            print(\"model saved\")\n",
    "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            \n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "\n",
    "    log_running_reward += current_ep_reward\n",
    "    log_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n",
    "    reward_list.append(current_ep_reward)\n",
    "    episode_list.append(i_episode)\n",
    "\n",
    "\n",
    "log_f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print total training time\n",
    "print(\"============================================================================================\")\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "print(\"Finished training at (GMT) : \", end_time)\n",
    "print(\"Total training time  : \", end_time - start_time)\n",
    "print(\"============================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce51c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_state1 = np.array([340 , 0, 9600*4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ea7a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actionvscrit():\n",
    "    crit_VIs = []\n",
    "    actions = []\n",
    "    for i in range(5,300,25):\n",
    "        test_state1 = np.array([i*150 , 0, 9600*4])\n",
    "        action = ppo_agent.select_action(test_state1)\n",
    "        action_mins_ = action_scalarization(-1,1,600*4,3600*4,action[0])\n",
    "        crit_VIs.append(i)\n",
    "        actions.append(action_mins_)\n",
    "    return crit_VIs,  actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71166d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_VIs,  actions = actionvscrit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "action_df = pd.DataFrame(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2ff829",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_df.to_csv('Actions_0.8-0.2_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc4171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1413f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(crit_VIs,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842a3403",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_test = ppo_agent.select_action(test_state1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b983870",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022103e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_scalarization(-1,1,600*4,3600*4,action_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_state2 = np.array([5600, 0, 1600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1c5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_test = ppo_agent.select_action(test_state2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d6b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_scalarization(-1,1,600,3600,action_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eab482",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_state2 = [9600, 0, 9600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6678741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VI_arrival(n_vi, n_crit, cvss_high_no):\n",
    "    high_list = [0.75,1]\n",
    "    low_list = [0.5,0.25,0.1]\n",
    "    med_list = [0.25,0.5]\n",
    "\n",
    "    cvss_ultra = [0.75,1]\n",
    "    cvss_high = [0.5,0.75]\n",
    "    cvss_med = [0.25,0.5,0.75,1]\n",
    "    cvss_low = [0.25,0.1]\n",
    "    \n",
    "    full_list = [0.5,0.25,0.75,1]\n",
    "\n",
    "    vi_list = []\n",
    "    total_vi = 0\n",
    "    while total_vi < n_vi:\n",
    "\n",
    "        for i in range(n_crit):\n",
    "            total_vi += 1\n",
    "            rand_no = 0.75\n",
    "            if  np.random.beta(3,1) < rand_no:\n",
    "                hvas = np.random.choice(high_list)\n",
    "            else:\n",
    "                hvas = np.random.choice(low_list)\n",
    "            \n",
    "            rand_no1 = 0.75\n",
    "            if np.random.beta(2,1) < rand_no1:\n",
    "                protection_level = np.random.choice(high_list)\n",
    "            else:\n",
    "                protection_level = np.random.choice(low_list)\n",
    "            rand_no2 = 0.75\n",
    "            if np.random.beta(0.5,1) < rand_no2:\n",
    "                ports = np.random.choice(high_list)\n",
    "            else:\n",
    "                ports = np.random.choice(low_list)\n",
    "            ids_info = 1\n",
    "#             cvss = np.random.choice(cvss_high)\n",
    "            rand_no4 = 0.75\n",
    "            if np.random.beta(1,1) > rand_no4:\n",
    "                cvss = np.random.choice([0.25,0.75,0.1,0.5])\n",
    "            else:\n",
    "                cvss = np.random.choice(low_list)\n",
    "                \n",
    "            exposure_score = 0.2 *  hvas + 0.2 * protection_level + 0.2* ports  + 0.2*ids_info + 0.2*cvss\n",
    "            mit_time = 150\n",
    "            vi = [hvas,protection_level, ports, ids_info, cvss, exposure_score, mit_time]\n",
    "            vi_list.append(vi)\n",
    "            \n",
    "\n",
    "        for i in range(cvss_high_no):\n",
    "            total_vi += 1\n",
    "            \n",
    "            hvas = np.random.choice([0.5,0.25,0.75,0.1])\n",
    "            protection_level = np.random.choice([0.5,0.25,0.75,0.1])\n",
    "            ports = np.random.choice([0.5,0.25,0.75,0.1])\n",
    "            ids_info = 0\n",
    "            cvss = np.random.choice(cvss_ultra)\n",
    "            exposure_score = 0.2 *  hvas + 0.2 * protection_level + 0.2* ports  + 0.2*ids_info + 0.2*cvss\n",
    "#             cvss = 1\n",
    "            mit_time = 150\n",
    "            vi = [hvas,protection_level, ports, ids_info, cvss,exposure_score, mit_time]\n",
    "            vi_list.append(vi)\n",
    "            \n",
    "        \n",
    "        VI_low_no = n_vi - (n_crit + cvss_high_no)\n",
    "        for i in range(VI_low_no):\n",
    "            total_vi += 1\n",
    "            hvas = np.random.choice(med_list)\n",
    "            protection_level = np.random.choice(med_list)\n",
    "            ports = np.random.choice(med_list)\n",
    "            ids_info = 0\n",
    "            cvss = np.random.choice(cvss_low)\n",
    "            exposure_score = 0.2 *  hvas + 0.2 * protection_level + 0.2* ports  + 0.2*ids_info + 0.2*cvss\n",
    "            mit_time = random.randint(25,100)\n",
    "            vi = [hvas,protection_level, ports, ids_info, cvss,exposure_score, mit_time]\n",
    "            vi_list.append(vi)\n",
    "\n",
    "    return np.array(vi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f4cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = VI_arrival(100, 25, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2fea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPO_model(t1,allocated_time):\n",
    "    \n",
    "    a = t1[:,5]\n",
    "    u = t1[:,-1]\n",
    "    \n",
    "    length = len(a)\n",
    "    model = Model('IT PROJECT')\n",
    "\n",
    "    x = model.addVars(length, vtype= GRB.BINARY)\n",
    "\n",
    "    model.addConstr((quicksum((u[i] * x[i] )for i in range(length))<= 13972.085))\n",
    "\n",
    "    model.setObjective((quicksum((x[i]*a[i]) for i in range(length))),GRB.MAXIMIZE)\n",
    "\n",
    "    result = model.optimize()\n",
    "\n",
    "    x_values = [int(x[i].x) for i in range(length)]\n",
    "    \n",
    "    k = []\n",
    "    for i in range(len(x_values)):\n",
    "        n= np.append(t1[i], x_values[i])\n",
    "        k.append(n)\n",
    "        \n",
    "    mit_list = []\n",
    "    unmit_list = []\n",
    "    for ind in k:\n",
    "        vi_inf = []\n",
    "        vi_inf.append(ind[0])\n",
    "        vi_inf.append(ind[1])\n",
    "        vi_inf.append(ind[2])\n",
    "        vi_inf.append(ind[3])\n",
    "        vi_inf.append(ind[4])\n",
    "        vi_inf.append(ind[5])\n",
    "        vi_inf.append(ind[6])\n",
    "        if ind[-1] == 1:\n",
    "            mit_list.append(vi_inf)\n",
    "        else:\n",
    "            unmit_list.append(vi_inf)\n",
    "            \n",
    "    return mit_list, unmit_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9552071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_implement_PPO(vi_list, action_, rem_action):\n",
    "    if action_ > rem_action:\n",
    "        action_ = rem_action\n",
    "    print(action_)\n",
    "        \n",
    "    mit_list, unmit_list = PPO_model(vi_list, action_)\n",
    "    \n",
    "    return mit_list, unmit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_implement(vi_list, action, rem_action, scheme):\n",
    "    if action > rem_action:\n",
    "        action = rem_action\n",
    "        \n",
    "        \n",
    "    if scheme == 'PPO':\n",
    "        sorted_list = vi_list[np.argsort(vi_list[:, -2])][::-1]\n",
    "    else:\n",
    "        sorted_list = vi_list[np.argsort(vi_list[:, -3])][::-1]\n",
    "    \n",
    " \n",
    "    if scheme == 'CVSS':\n",
    "        mit_vi_list = []\n",
    "        for i in sorted_list:\n",
    "\n",
    "            time_spent = i[-1]\n",
    "            action = action - time_spent\n",
    "\n",
    "            mit_vi_list.append(i)\n",
    "            sorted_list = np.delete(sorted_list, [0], 0)\n",
    "            if action <= 0 or len(sorted_list)<= 0:\n",
    "                break\n",
    "               \n",
    "    return mit_vi_list, sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_calc(mit_list):\n",
    "    \n",
    "    n_hvas = 0\n",
    "    n_low_prot = 0\n",
    "    n_port = 0\n",
    "    n_ids = 0\n",
    "    n_CVSS = 0\n",
    "    \n",
    "    for i in mit_list:\n",
    "        if i[0] >= 0.75:\n",
    "            n_hvas += 1\n",
    "            \n",
    "        if i[1] >= 0.75:\n",
    "            n_low_prot += 1\n",
    "            \n",
    "        if i[2] >= 0.75:\n",
    "            n_port += 1\n",
    "        \n",
    "        if i[3] >= 0.75:\n",
    "            n_ids += 1\n",
    "        \n",
    "        if i[4] >= 0.75:\n",
    "            n_CVSS += 1\n",
    "            \n",
    "    return n_hvas,n_low_prot, n_port,n_ids, n_CVSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a886d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gurobipy import Model, quicksum, GRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfd56e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9034335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VPSS_data_gen(t1):\n",
    "    VI_VPSS = []\n",
    "    VPSS_list = []\n",
    "    time_list = []\n",
    "    for v in t1:\n",
    "        VPSS = 0.25 * v[0] + 0.25 * v[1] + 0.25 * v[2] + 0.25 * v[4]\n",
    "        VPSS_list.append(VPSS)\n",
    "        time_list.append(v[-1])\n",
    "        v = np.append(v,VPSS)\n",
    "\n",
    "        VI_VPSS.append(v)\n",
    "        \n",
    "    return VPSS_list,time_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78049266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VULCON_data_gen(t1):\n",
    "    VI_VPSS = []\n",
    "    VPSS_list = []\n",
    "    time_list = []\n",
    "    for v in t1:\n",
    "        VPSS = 0.2 * v[-1] + 0.2 * v[-2] + 0.3 * v[0] + 0.1 * v[-4]\n",
    "        VPSS_list.append(VPSS)\n",
    "        time_list.append(v[-3])\n",
    "        v = np.append(v,VPSS)\n",
    "\n",
    "        VI_VPSS.append(v)\n",
    "        \n",
    "    return VPSS_list,time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667aab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VPSS_model(t1):\n",
    "    \n",
    "    a, u = VPSS_data_gen(t1)\n",
    "    \n",
    "    length = len(a)\n",
    "    model = Model('IT PROJECT')\n",
    "\n",
    "    x = model.addVars(length, vtype= GRB.BINARY)\n",
    "\n",
    "    model.addConstr((quicksum((u[i] * x[i] )for i in range(length))<= 9600))\n",
    "\n",
    "    model.setObjective((quicksum((x[i]*a[i]) for i in range(length))),GRB.MAXIMIZE)\n",
    "\n",
    "    result = model.optimize()\n",
    "\n",
    "    x_values = [int(x[i].x) for i in range(length)]\n",
    "    \n",
    "    k = []\n",
    "    for i in range(len(x_values)):\n",
    "        n= np.append(t1[i], x_values[i])\n",
    "        k.append(n)\n",
    "        \n",
    "    mit_list = []\n",
    "    unmit_list = []\n",
    "    for ind in k:\n",
    "        vi_inf = []\n",
    "        vi_inf.append(ind[0])\n",
    "        vi_inf.append(ind[1])\n",
    "        vi_inf.append(ind[2])\n",
    "        vi_inf.append(ind[3])\n",
    "        vi_inf.append(ind[4])\n",
    "        vi_inf.append(ind[5])\n",
    "        vi_inf.append(ind[6])\n",
    "        if ind[-1] == 1:\n",
    "            mit_list.append(vi_inf)\n",
    "        else:\n",
    "            unmit_list.append(vi_inf)\n",
    "            \n",
    "    return mit_list, unmit_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c43e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VULCON_model(t1):\n",
    "    \n",
    "    a, u = VULCON_data_gen(t1)\n",
    "    \n",
    "    length = len(a)\n",
    "    model = Model('IT PROJECT')\n",
    "\n",
    "    x = model.addVars(length, vtype= GRB.BINARY)\n",
    "\n",
    "    model.addConstr((quicksum((u[i] * x[i] )for i in range(length))<= 9600))\n",
    "\n",
    "    model.setObjective((quicksum((x[i]*a[i]) for i in range(length))),GRB.MAXIMIZE)\n",
    "\n",
    "    result = model.optimize()\n",
    "\n",
    "    x_values = [int(x[i].x) for i in range(length)]\n",
    "    \n",
    "    k = []\n",
    "    for i in range(len(x_values)):\n",
    "        n= np.append(t1[i], x_values[i])\n",
    "        k.append(n)\n",
    "        \n",
    "    mit_list = []\n",
    "    unmit_list = []\n",
    "    for ind in k:\n",
    "        vi_inf = []\n",
    "        vi_inf.append(ind[0])\n",
    "        vi_inf.append(ind[1])\n",
    "        vi_inf.append(ind[2])\n",
    "        vi_inf.append(ind[3])\n",
    "        vi_inf.append(ind[4])\n",
    "        vi_inf.append(ind[5])\n",
    "        vi_inf.append(ind[6])\n",
    "        vi_inf.append(ind[7])\n",
    "        vi_inf.append(ind[8])\n",
    "        if ind[-1] == 1:\n",
    "            mit_list.append(vi_inf)\n",
    "        else:\n",
    "            unmit_list.append(vi_inf)\n",
    "            \n",
    "    return mit_list, unmit_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hvas_PPO_list = []\n",
    "n_low_prot_PPO_list = []\n",
    "n_port_PPO_list = []\n",
    "n_ids_PPO_list = []\n",
    "n_CVSS_PPO_list = []\n",
    "\n",
    "n_hvas_CVSS_list = []\n",
    "n_low_prot_CVSS_list = []\n",
    "n_port_CVSS_list = []\n",
    "n_ids_CVSS_list = []\n",
    "n_CVSS_CVSS_list = []\n",
    "\n",
    "n_hvas_PPO_list_week = []\n",
    "n_low_prot_PPO_list_week = []\n",
    "n_port_PPO_list_week = []\n",
    "n_ids_PPO_list_week = []\n",
    "n_CVSS_PPO_list_week = []\n",
    "\n",
    "n_hvas_CVSS_list_week = []\n",
    "n_low_prot_CVSS_list_week = []\n",
    "n_port_CVSS_list_week = []\n",
    "n_ids_CVSS_list_week = []\n",
    "n_CVSS_CVSS_list_week = []\n",
    "\n",
    "n_hvas_VPSS_list_week = []\n",
    "n_low_prot_VPSS_list_week = []\n",
    "n_port_VPSS_list_week = []\n",
    "n_ids_VPSS_list_week = []\n",
    "n_CVSS_VPSS_list_week = []\n",
    "\n",
    "n_hvas_VULCON_list_week = []\n",
    "n_low_prot_VULCON_list_week = []\n",
    "n_port_VULCON_list_week = []\n",
    "n_ids_VULCON_list_week = []\n",
    "n_CVSS_VULCON_list_week = []\n",
    "\n",
    "for epi in range(13):\n",
    "    \n",
    "    rem_action = 9600*4\n",
    "\n",
    "    \n",
    "#     total_vi_week1 = [np.random.poisson(220), np.random.poisson(140), np.random.poisson(120), np.random.poisson(240),0]\n",
    "#     total_vi_week2 = [np.random.poisson(135), np.random.poisson(140), np.random.poisson(220), np.random.poisson(120),0]\n",
    "#     total_vi_week3 = [np.random.poisson(120), np.random.poisson(220), np.random.poisson(240), np.random.poisson(140),0]\n",
    "#     total_vi_week4 = [np.random.poisson(220), np.random.poisson(240), np.random.poisson(120), np.random.poisson(130),0]\n",
    "    \n",
    "#     total_pat_sel = [total_vi_week1,total_vi_week2,total_vi_week3,total_vi_week4]\n",
    "    \n",
    "#     crit_week1 = [random.randint(4,20), random.randint(40,60), random.randint(80,100), random.randint(4,20),0]\n",
    "#     crit_week2 = [ random.randint(40,60),random.randint(4,20),  random.randint(4,20),random.randint(80,100),0]\n",
    "#     crit_week3 = [random.randint(4,20), random.randint(4,20),  random.randint(80,100),random.randint(40,60),0]\n",
    "#     crit_week4 = [random.randint(80,100), random.randint(4,20), random.randint(40,60),  random.randint(4,20),0]\n",
    "    \n",
    "#     crit_pat_sel = [crit_week1,crit_week2,crit_week3,crit_week4]\n",
    "    \n",
    "    total_vi_week1 = [np.random.poisson(400), np.random.poisson(500), np.random.poisson(500), np.random.poisson(600),0]\n",
    "    total_vi_week2 = [np.random.poisson(450), np.random.poisson(550), np.random.poisson(650), np.random.poisson(350),0]\n",
    "    total_vi_week3 = [np.random.poisson(450), np.random.poisson(500), np.random.poisson(600), np.random.poisson(575),0]\n",
    "    total_vi_week4 = [np.random.poisson(400), np.random.poisson(500), np.random.poisson(650), np.random.poisson(500),0]\n",
    "    \n",
    "    total_pat_sel = [total_vi_week1,total_vi_week2,total_vi_week3,total_vi_week4]\n",
    "    \n",
    "    crit_week1 = [random.randint(140,200), random.randint(180,200), random.randint(140,200), random.randint(140,200),0]\n",
    "    crit_week2 = [random.randint(180,200),random.randint(140,220),random.randint(180,200), random.randint(140,150),0]\n",
    "    crit_week3 = [random.randint(140,200),random.randint(180,200), random.randint(140,160), random.randint(140,150), 0]\n",
    "    crit_week4 = [random.randint(140,200), random.randint(240,260), random.randint(180,220), random.randint(140,150),0]\n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    crit_pat_sel = [crit_week1,crit_week2,crit_week3,crit_week4]\n",
    "    \n",
    "\n",
    "    \n",
    "    crit_week = random.choice(crit_pat_sel)\n",
    "    total_vi_week = random.choice(total_pat_sel)\n",
    "    \n",
    "    init_arrival = VI_arrival(total_vi_week[0], crit_week[0], crit_week[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    state = [crit_week[0]*150 , 0, 9600*4]\n",
    "    \n",
    "    rem_action_PPO = rem_action\n",
    "    rem_action_CVSS = rem_action\n",
    "    \n",
    "    \n",
    "    total_n_hvas_PPO = 0\n",
    "    total_n_low_prot_PPO = 0\n",
    "    total_n_port_PPO = 0\n",
    "    total_n_ids_PPO = 0\n",
    "    total_n_CVSS_PPO = 0\n",
    "\n",
    "    total_n_hvas_CVSS = 0\n",
    "    total_n_low_prot_CVSS = 0\n",
    "    total_n_port_CVSS = 0\n",
    "    total_n_ids_CVSS = 0\n",
    "    total_n_CVSS_CVSS = 0\n",
    "    \n",
    "    total_crit = crit_week[0]\n",
    "    VI_list_PPO = init_arrival\n",
    "    VI_list_CVSS = init_arrival\n",
    "    VI_list_VPSS = init_arrival\n",
    "    init_arrival_VULCON = []\n",
    "    for item in init_arrival:\n",
    "        age = random.random()\n",
    "        \n",
    "        persistense = 0\n",
    "        item = np.append(item,age)\n",
    "        item = np.append(item,persistense)\n",
    "        \n",
    "        init_arrival_VULCON.append(item)\n",
    "    \n",
    "    VI_list_VULCON = init_arrival_VULCON\n",
    "        \n",
    "    for week in range(4):\n",
    "\n",
    "        action_PPO = ppo_agent.select_action(state)\n",
    "        \n",
    "        if week != 3:\n",
    "            action_PPO_mins = action_scalarization(-1,1,600*4,3600*4,action_PPO[0])\n",
    "        else:\n",
    "            action_PPO_mins = rem_action_PPO\n",
    "        \n",
    "        \n",
    "        rem_crit = total_crit - int(action_PPO_mins/150)\n",
    "        \n",
    "        action_CVSS = 2400*4\n",
    "        \n",
    "        if action_PPO < rem_action_PPO:\n",
    "            rem_action_PPO -= action_PPO_mins\n",
    "        else:\n",
    "            rem_action_PPO = 0\n",
    "            \n",
    "        rem_action_CVSS -= action_CVSS\n",
    "        \n",
    "        print(action_PPO_mins)\n",
    "\n",
    "        mit_vi_list_PPO, unmit_vi_list_PPO = action_implement_PPO(VI_list_PPO, action_PPO_mins, rem_action_PPO)\n",
    "        \n",
    "        \n",
    "        mit_vi_list_CVSS, unmit_vi_list_CVSS = action_implement(VI_list_CVSS, action_CVSS, rem_action_CVSS, 'CVSS')\n",
    "        \n",
    "        mit_vi_list_VPSS, unmit_vi_list_VPSS = VPSS_model(VI_list_VPSS)\n",
    "        \n",
    "        mit_vi_list_VULCON, unmit_vi_list_VULCON = VULCON_model(VI_list_VULCON)\n",
    "        \n",
    "        for vi in unmit_vi_list_VULCON:\n",
    "            vi[-1] += 0.25\n",
    "\n",
    "        n_hvas_PPO,n_low_prot_PPO, n_port_PPO,n_ids_PPO, n_CVSS_PPO = attribute_calc(mit_vi_list_PPO)\n",
    "        \n",
    "        n_hvas_CVSS,n_low_prot_CVSS, n_port_CVSS,n_ids_CVSS, n_CVSS_CVSS = attribute_calc(mit_vi_list_CVSS)\n",
    "        \n",
    "        n_hvas_VPSS,n_low_prot_VPSS, n_port_VPSS,n_ids_VPSS, n_CVSS_VPSS = attribute_calc(mit_vi_list_VPSS)\n",
    "        \n",
    "        n_hvas_VULCON,n_low_prot_VULCON, n_port_VULCON,n_ids_VULCON, n_CVSS_VULCON = attribute_calc(mit_vi_list_VULCON)\n",
    "        \n",
    "        n_hvas_PPO_list_week.append(n_hvas_PPO)\n",
    "        n_low_prot_PPO_list_week.append(n_low_prot_PPO)\n",
    "        n_port_PPO_list_week.append(n_port_PPO)\n",
    "        n_ids_PPO_list_week.append(n_ids_PPO)\n",
    "        n_CVSS_PPO_list_week.append(n_CVSS_PPO)\n",
    "        \n",
    "        n_hvas_CVSS_list_week.append(n_hvas_CVSS)\n",
    "        n_low_prot_CVSS_list_week.append(n_low_prot_CVSS)\n",
    "        n_port_CVSS_list_week.append(n_port_CVSS)\n",
    "        n_ids_CVSS_list_week.append(n_ids_CVSS)\n",
    "        n_CVSS_CVSS_list_week.append(n_CVSS_CVSS)\n",
    "        \n",
    "        n_hvas_VPSS_list_week.append(n_hvas_VPSS)\n",
    "        n_low_prot_VPSS_list_week.append(n_low_prot_VPSS)\n",
    "        n_port_VPSS_list_week.append(n_port_VPSS)\n",
    "        n_ids_VPSS_list_week.append(n_ids_VPSS)\n",
    "        n_CVSS_VPSS_list_week.append(n_CVSS_VPSS)\n",
    "        \n",
    "        n_hvas_VULCON_list_week.append(n_hvas_VULCON)\n",
    "        n_low_prot_VULCON_list_week.append(n_low_prot_VULCON)\n",
    "        n_port_VULCON_list_week.append(n_port_VULCON)\n",
    "        n_ids_VULCON_list_week.append(n_ids_VULCON)\n",
    "        n_CVSS_VULCON_list_week.append(n_CVSS_VULCON)\n",
    "        \n",
    "        \n",
    "        total_n_hvas_PPO += n_hvas_PPO\n",
    "        total_n_low_prot_PPO += n_low_prot_PPO\n",
    "        total_n_port_PPO += n_port_PPO\n",
    "        total_n_ids_PPO += n_ids_PPO\n",
    "        total_n_CVSS_PPO += n_CVSS_PPO\n",
    "\n",
    "        \n",
    "        \n",
    "        total_n_hvas_CVSS += n_hvas_CVSS\n",
    "        total_n_low_prot_CVSS += n_low_prot_CVSS\n",
    "        total_n_port_CVSS += n_port_CVSS\n",
    "        total_n_ids_CVSS += n_ids_CVSS\n",
    "        total_n_CVSS_CVSS += n_CVSS_CVSS\n",
    "        \n",
    "        if week == 3:\n",
    "            break\n",
    "        arrival = VI_arrival(total_vi_week[week + 1], crit_week[week + 1], crit_week[week + 1])\n",
    "        \n",
    "        if len(unmit_vi_list_PPO) > 0:\n",
    "            VI_list_PPO = np.vstack((arrival , unmit_vi_list_PPO))\n",
    "        else:\n",
    "            VI_list_PPO = arrival\n",
    "            \n",
    "        if len(unmit_vi_list_CVSS) > 0:\n",
    "            VI_list_CVSS = np.vstack((arrival , unmit_vi_list_CVSS))\n",
    "        else:\n",
    "            VI_list_CVSS = arrival\n",
    "            \n",
    "        if len(unmit_vi_list_VPSS) > 0:\n",
    "            VI_list_VPSS = np.vstack((arrival , unmit_vi_list_VPSS))\n",
    "        else:\n",
    "            VI_list_VPSS = arrival\n",
    "            \n",
    "            \n",
    "        arrival_VULCON = []\n",
    "        for item in arrival:\n",
    "            age = random.random()\n",
    "\n",
    "            persistense = 0\n",
    "            item = np.append(item,age)\n",
    "            item = np.append(item,persistense)\n",
    "\n",
    "            arrival_VULCON.append(item)\n",
    "            \n",
    "        len(arrival_VULCON)\n",
    "            \n",
    "        if len(unmit_vi_list_VULCON) > 0:\n",
    "            VI_list_VULCON = np.vstack((arrival_VULCON , unmit_vi_list_VULCON))\n",
    "        else:\n",
    "            VI_list_VULCON = arrival_VULCON\n",
    "        \n",
    "        total_crit = crit_week[week + 1] + rem_crit\n",
    "        state = [(total_crit)*150, week + 1, rem_action_PPO]\n",
    "        \n",
    "\n",
    "    \n",
    "    n_hvas_PPO_list.append(total_n_hvas_PPO)\n",
    "    n_low_prot_PPO_list.append(total_n_low_prot_PPO)\n",
    "    n_port_PPO_list.append(total_n_port_PPO)\n",
    "    n_ids_PPO_list.append(total_n_ids_PPO)\n",
    "    n_CVSS_PPO_list.append(total_n_CVSS_PPO)\n",
    "\n",
    "    n_hvas_CVSS_list.append(total_n_hvas_CVSS)\n",
    "    n_low_prot_CVSS_list.append(total_n_low_prot_CVSS)\n",
    "    n_port_CVSS_list.append(total_n_port_CVSS)\n",
    "    n_ids_CVSS_list.append(total_n_ids_CVSS)\n",
    "    n_CVSS_CVSS_list.append(total_n_CVSS_CVSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08507799",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hvas_PPO_list_cum = np.cumsum(n_hvas_PPO_list_week)\n",
    "n_hvas_CVSS_list_cum = np.cumsum(n_hvas_CVSS_list_week)\n",
    "n_hvas_VPSS_list_cum = np.cumsum(n_hvas_VPSS_list_week)\n",
    "n_hvas_VULCON_list_cum = np.cumsum(n_hvas_VULCON_list_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_CVSS_PPO_list_cum = np.cumsum(n_CVSS_PPO_list_week)\n",
    "n_CVSS_CVSS_list_cum = np.cumsum(n_CVSS_CVSS_list_week)\n",
    "n_CVSS_VPSS_list_cum = np.cumsum(n_CVSS_VPSS_list_week)\n",
    "n_CVSS_VULCON_list_cum = np.cumsum(n_CVSS_VULCON_list_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_low_prot_PPO_list_cum = np.cumsum(n_low_prot_PPO_list_week)\n",
    "n_low_prot_CVSS_list_cum = np.cumsum(n_low_prot_CVSS_list_week)\n",
    "n_low_prot_VPSS_list_cum = np.cumsum(n_low_prot_VPSS_list_week)\n",
    "n_low_prot_VULCON_list_cum = np.cumsum(n_low_prot_VULCON_list_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea9e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_port_PPO_list_cum = np.cumsum(n_port_PPO_list_week)\n",
    "n_port_CVSS_list_cum = np.cumsum(n_port_CVSS_list_week)\n",
    "n_port_VPSS_list_cum = np.cumsum(n_port_VPSS_list_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335dc26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ids_PPO_list_cum = np.cumsum(n_ids_PPO_list_week)\n",
    "n_ids_CVSS_list_cum = np.cumsum(n_ids_CVSS_list_week)\n",
    "n_ids_VPSS_list_cum = np.cumsum(n_ids_VPSS_list_week)\n",
    "n_ids_VULCON_list_cum = np.cumsum(n_ids_VULCON_list_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9531316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Iteration = [i for i in range(1,53)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e069fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [ 'DRL-Agent','CVSS value-based', 'VPSS', 'VULCON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ace54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Iteration, n_hvas_PPO_list_cum , color='g')\n",
    "plt.plot(Iteration, n_hvas_CVSS_list_cum , color='r')\n",
    "plt.plot(Iteration, n_hvas_VPSS_list_cum , color='y')\n",
    "plt.plot(Iteration, n_hvas_VULCON_list_cum , color='b')\n",
    "plt.legend(labels = label, loc = 'best')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Number of Selected Vulnerabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f8103",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Iteration, n_CVSS_PPO_list_cum , color='g')\n",
    "plt.plot(Iteration, n_CVSS_CVSS_list_cum , color='r')\n",
    "plt.plot(Iteration, n_CVSS_VPSS_list_cum , color='y')\n",
    "plt.plot(Iteration, n_CVSS_VULCON_list_cum , color='b')\n",
    "plt.legend(labels = label, loc = 'best')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Number of Selected Vulnerabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Iteration, n_low_prot_PPO_list_cum , color='g')\n",
    "plt.plot(Iteration, n_low_prot_CVSS_list_cum , color='r')\n",
    "plt.plot(Iteration, n_low_prot_VPSS_list_cum , color='y')\n",
    "plt.plot(Iteration, n_low_prot_VULCON_list_cum , color='b')\n",
    "plt.legend(labels = label, loc = 'best')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Number of Selected Vulnerabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dc4026",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Iteration, n_port_PPO_list_cum , color='g')\n",
    "plt.plot(Iteration, n_port_CVSS_list_cum , color='r')\n",
    "plt.plot(Iteration, n_low_prot_VPSS_list_cum , color='y')\n",
    "plt.plot(Iteration, n_low_prot_VULCON_list_cum , color='b')\n",
    "plt.legend(labels = label, loc = 'best')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Number of Selected Vulnerabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2038a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Iteration, n_ids_PPO_list_cum , color='g')\n",
    "plt.plot(Iteration, n_ids_CVSS_list_cum , color='r')\n",
    "plt.plot(Iteration, n_ids_VPSS_list_cum , color='y')\n",
    "plt.plot(Iteration, n_ids_VULCON_list_cum , color='b')\n",
    "plt.legend(labels = label, loc = 'best')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Number of Selected Vulnerabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df69feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a17f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c568a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119e3345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87586d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
